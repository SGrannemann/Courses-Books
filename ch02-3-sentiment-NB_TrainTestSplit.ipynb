{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "59507d4b7a424780c7daeb44aadd3841b613d4d87d8fb5cbad99945b5c4a9b7a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an example for a naive bayes sentiment analyzer from the book \"Natural Language Processing in Action\" by Lane, Howard and Hapke.\n",
    "# i only added a few more comments and made it work without installing the NLPIA package.\n",
    "\n",
    "import pandas as pd\n",
    "from nltk.tokenize import casual_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# read in the text file \n",
    "movies = pd.read_csv('movieReviewSnippets_GroundTruth.txt', sep='\\t', index_col=0, names=['sentiment', 'text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   sentiment                                               text\n",
       "1   2.266667  The Rock is destined to be the 21st Century's ...\n",
       "2   3.533333  The gorgeously elaborate continuation of ''The...\n",
       "3  -0.600000                     Effective but too tepid biopic\n",
       "4   1.466667  If you sometimes like to go to the movies to h...\n",
       "5   1.733333  Emerges as something rare, an issue movie that..."
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2.266667</td>\n      <td>The Rock is destined to be the 21st Century's ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.533333</td>\n      <td>The gorgeously elaborate continuation of ''The...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.600000</td>\n      <td>Effective but too tepid biopic</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.466667</td>\n      <td>If you sometimes like to go to the movies to h...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.733333</td>\n      <td>Emerges as something rare, an issue movie that...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# lets take a quick look at our data\n",
    "movies.describe()\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10605, 20756)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# the reviews are rated from -4 to 4\n",
    "\n",
    "# let's convert them into the bag-of-words representation\n",
    "\n",
    "bags_of_words = []\n",
    "for text in movies.text:\n",
    "    bags_of_words.append(Counter(casual_tokenize(text))) # Counter creates a dict with token: count of token\n",
    "\n",
    "# lets create a Dataframe of bows\n",
    "df_bows = pd.DataFrame.from_records(bags_of_words)\n",
    "# fill all NaN with zero so we can convert them to int\n",
    "df_bows = df_bows.fillna(0).astype(int)\n",
    "\n",
    "# lets see how large our bow table has become\n",
    "df_bows.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 10605 reviews include 20756 different tokens\n",
    "# what threw me off at first is: the bag-of-words conversion via the Counter returns dense representations - only actually present tokens are included in the bow representation of a single review.\n",
    "# these become sparse by creating the dataframe - from_records creates one column per unique key in the list of bow representations and fills each column with missing values with NaN (which we then replaced by 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3643192939004942\n",
      "2.878633376550765\n",
      "2.4929341875533484\n"
     ]
    }
   ],
   "source": [
    "# lets start with the model itself\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# split the dataset into training and validation set\n",
    "train_x, val_x, train_y, val_y = train_test_split(df_bows, movies.sentiment, random_state=1)\n",
    "\n",
    "nb = nb.fit(train_x, train_y > 0) # convert the train_y to a boolean value\n",
    "\n",
    "\n",
    "# training MAE\n",
    "train_predictions = nb.predict(train_x)\n",
    "training_mae = mean_absolute_error(train_predictions * 8 - 4, train_y)\n",
    "print(training_mae)\n",
    "\n",
    "# predict the validation set\n",
    "val_predictions = nb.predict(val_x) \n",
    "# calculate MAE for the validation data set\n",
    "val_mae = mean_absolute_error(val_predictions * 8 - 4, val_y)\n",
    "print(val_mae)\n",
    "\n",
    "\n",
    "# now lets predict the whole dataset and add the prediction to the dataframe.\n",
    "movies['predicted_sentiment'] = nb.predict(df_bows) * 8 - 4 #convert binary classification to -4 or 4 for comparison.\n",
    "\n",
    "complete_mae = mean_absolute_error(movies['predicted_sentiment'], movies.sentiment)\n",
    "print(complete_mae)\n",
    "movies['error'] = (movies.predicted_sentiment - movies.sentiment).abs()\n",
    "\n",
    "\n",
    "# as we can see, the authors of the book were correct: one should split training and validation sets. \n",
    "# the MAE is much larger for the predictions on the validation data set.\n",
    "# the MAE is in this case potentially not the best metric - we built a classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   sentiment  predicted_sentiment  sentiment_ispositive  predicted_ispositive\n",
       "1   2.266667                    4                     1                     1\n",
       "2   3.533333                    4                     1                     1\n",
       "3  -0.600000                   -4                     0                     0\n",
       "4   1.466667                    4                     1                     1\n",
       "5   1.733333                    4                     1                     1\n",
       "6   2.533333                    4                     1                     1\n",
       "7   2.466667                    4                     1                     1\n",
       "8   1.266667                   -4                     1                     0"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>predicted_sentiment</th>\n      <th>sentiment_ispositive</th>\n      <th>predicted_ispositive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2.266667</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.533333</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.600000</td>\n      <td>-4</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.466667</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.733333</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2.533333</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2.466667</td>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1.266667</td>\n      <td>-4</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "movies['sentiment_ispositive'] = (movies.sentiment > 0 ).astype(int)\n",
    "movies['predicted_ispositive'] = (movies.predicted_sentiment > 0 ).astype(int)\n",
    "movies['sentiment predicted_sentiment sentiment_ispositive predicted_ispositive'.split()].head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.902970297029703\n0.9454293977115554\n0.7756410256410257\n"
     ]
    }
   ],
   "source": [
    "print((movies.predicted_ispositive == movies.sentiment_ispositive).sum() / len(movies))\n",
    "# training set only\n",
    "print((train_predictions == (train_y > 0)).sum() / len(train_predictions))\n",
    "\n",
    "# same calculation for the validation set only:\n",
    "print((val_predictions == (val_y > 0)).sum() / len(val_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# that means we get roughly 78 % of the labels correct in the validation data set\n",
    "# instead of 90 % in the case of the complete dataset or 95 % on the training set.\n",
    "# next we should have a look at a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     sentiment                                               text\n",
      "1_1      -0.90  troubleshooting ad-2500 and ad-2600 no picture...\n",
      "1_2      -0.15  repost from january 13, 2004 with a better fit...\n",
      "1_3      -0.20  does your apex dvd player only play dvd audio ...\n",
      "1_4      -0.10  or does it play audio and video but scrolling ...\n",
      "1_5      -0.50  before you try to return the player or waste h...\n",
      "   troubleshooting   ad    -  2500  and  2600   no  picture  scrolling    b  \\\n",
      "0              1.0  2.0  2.0   1.0  1.0   1.0  1.0      1.0        1.0  1.0   \n",
      "1              NaN  NaN  NaN   NaN  NaN   NaN  NaN      NaN        NaN  NaN   \n",
      "2              NaN  NaN  NaN   NaN  NaN   NaN  NaN      NaN        NaN  NaN   \n",
      "3              NaN  NaN  NaN   NaN  2.0   NaN  NaN      NaN        1.0  NaN   \n",
      "4              1.0  NaN  NaN   NaN  NaN   NaN  NaN      NaN        NaN  NaN   \n",
      "\n",
      "   ...  undone  warrranty  expire  expired  voids  develops  soldier  serving  \\\n",
      "0  ...     NaN        NaN     NaN      NaN    NaN       NaN      NaN      NaN   \n",
      "1  ...     NaN        NaN     NaN      NaN    NaN       NaN      NaN      NaN   \n",
      "2  ...     NaN        NaN     NaN      NaN    NaN       NaN      NaN      NaN   \n",
      "3  ...     NaN        NaN     NaN      NaN    NaN       NaN      NaN      NaN   \n",
      "4  ...     NaN        NaN     NaN      NaN    NaN       NaN      NaN      NaN   \n",
      "\n",
      "   baghdad  harddisk  \n",
      "0      NaN       NaN  \n",
      "1      NaN       NaN  \n",
      "2      NaN       NaN  \n",
      "3      NaN       NaN  \n",
      "4      NaN       NaN  \n",
      "\n",
      "[5 rows x 5687 columns]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['The', 'Rock', 'is', 'destined', 'to', 'be', 'the', '21st', 'Century's',\n",
       "       'new',\n",
       "       ...\n",
       "       'sligtly', 'owner', '81', 'defectively', 'warrranty', 'expire',\n",
       "       'expired', 'voids', 'baghdad', 'harddisk'],\n",
       "      dtype='object', length=23302)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# lets see how our model performs on a different dataset\n",
    "products = pd.read_csv('amazonReviewSnippets_GroundTruth.txt', sep='\\t', index_col=0, names=['sentiment', 'text'] )\n",
    "\n",
    "bags_of_words = []\n",
    "# we convert these reviews to BoW in the same way as the movie reviews\n",
    "for text in products.text:\n",
    "    bags_of_words.append(Counter(casual_tokenize(text)))\n",
    "df_product_bows = pd.DataFrame.from_records(bags_of_words)\n",
    "\n",
    "df_product_bows = df_product_bows.fillna(0).astype(int)\n",
    "df_all_bows = df_bows.append(df_product_bows)\n",
    "df_all_bows.columns\n",
    "# we now have more columns - this corresponds to more tokens in the product reviews than in the movie reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   The  Rock  is  destined  to  be  the  21st  Century's  new  ...  Ill  \\\n0    0     0   0         0   0   0    0     0          0    0  ...    0   \n1    0     0   0         0   0   0    0     0          0    0  ...    0   \n2    0     0   0         0   0   0    0     0          0    0  ...    0   \n3    0     0   0         0   0   0    0     0          0    0  ...    0   \n4    0     0   0         0   1   0    2     0          0    0  ...    0   \n\n   slummer  Rashomon  dipsticks  Bearable  Staggeringly  ’  ve  muttering  \\\n0        0         0          0         0             0  0   0          0   \n1        0         0          0         0             0  0   0          0   \n2        0         0          0         0             0  0   0          0   \n3        0         0          0         0             0  0   0          0   \n4        0         0          0         0             0  0   0          0   \n\n   dissing  \n0        0  \n1        0  \n2        0  \n3        0  \n4        0  \n\n[5 rows x 20756 columns]\n"
     ]
    }
   ],
   "source": [
    "df_product_bows = df_all_bows.iloc[len(movies):][df_bows.columns]\n",
    "# we need to fill the NaN again that stem from the tokens available in the movie reviews, but not the product reviews.\n",
    "df_product_bows = df_product_bows.fillna(0).astype(int)\n",
    "print(df_product_bows.head())\n",
    "products['ispos'] = (products.sentiment > 0).astype(int)\n",
    "products['predicted_ispositive'] = nb.predict(df_product_bows.values).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5572476029328821"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "(products.predicted_ispositive == products.ispos).sum() / len(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}