{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "import string\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "datapath = Path.cwd() / Path('datasets') / Path('movie reviews')\n",
    "train_path = datapath / Path('train')\n",
    "test_path = datapath / Path('test')\n",
    "\n",
    "train_paths_neg = list((train_path / Path('neg')).glob('*.txt')) \n",
    "train_paths_neg = [str(path) for path in train_paths_neg]\n",
    "\n",
    "train_paths_pos = list((train_path / Path('pos')).glob('*.txt'))\n",
    "train_paths_pos = [str(path) for path in train_paths_pos]\n",
    "\n",
    "test_paths_neg = list((test_path / Path('neg')).glob('*.txt')) \n",
    "test_paths_neg = [str(path) for path in test_paths_neg]\n",
    "test_paths_pos = list((test_path / Path('pos')).glob('*.txt'))\n",
    "test_paths_pos = [str(path) for path in test_paths_pos]\n",
    "# split test and validation set\n",
    "validation_paths_neg = test_paths_neg[:7500]\n",
    "validation_paths_pos = test_paths_pos[:7500]\n",
    "test_paths_neg = test_paths_neg[7500:]\n",
    "test_paths_pos = test_paths_pos[7500:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create efficient TF dataset\n",
    "def create_imdb_dataset(*, filepaths_positive, filepaths_negative, n_readers=5):\n",
    "    # read in as TextLineDataset and add label to it\n",
    "    dataset_neg = tf.data.TextLineDataset(filepaths_negative, num_parallel_reads=n_readers)\n",
    "    dataset_neg = dataset_neg.map(lambda review: (review, 0))\n",
    "    dataset_pos = tf.data.TextLineDataset(filepaths_positive, num_parallel_reads=n_readers)\n",
    "    dataset_pos = dataset_pos.map(lambda review: (review, 1))\n",
    "    return tf.data.Dataset.concatenate(dataset_neg, dataset_pos)\n",
    "\n",
    "batch_size = 32\n",
    "train_set = create_imdb_dataset(filepaths_positive=train_paths_pos, filepaths_negative=train_paths_neg).shuffle(25000).batch(batch_size).prefetch(1)\n",
    "test_set = create_imdb_dataset(filepaths_positive=test_paths_pos, filepaths_negative=test_paths_neg).batch(batch_size).prefetch(1)\n",
    "valid_set = create_imdb_dataset(filepaths_positive=validation_paths_pos, filepaths_negative=validation_paths_neg).batch(batch_size).prefetch(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextVectorization preprocessing layer\n",
    "def standardize_text(input):\n",
    "    lower = tf.strings.lower(input)\n",
    "    stripped_linebreaks = tf.strings.regex_replace(lower, '<br />', ' ')\n",
    "    return tf.strings.regex_replace(stripped_linebreaks, f'[{re.escape(string.punctuation)}]', '')\n",
    "\n",
    "vocab_size = 10000\n",
    "sequence_length = 100\n",
    "text_vectorization_layer = keras.layers.TextVectorization(standardize=standardize_text, max_tokens=vocab_size, output_sequence_length=sequence_length)\n",
    "text_vectorization_layer.adapt(train_set.map(lambda text, label: text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binary classifier model\n",
    "embedding_dim = 16\n",
    "\n",
    "# compute mean embedding and multiply by sqrt(number of words)\n",
    "def compute_mean_embedding(inputs):\n",
    "    not_pad = tf.math.count_nonzero(inputs, axis=-1)\n",
    "    n_words = tf.math.count_nonzero(not_pad, axis=-1, keepdims=True)    \n",
    "    sqrt_n_words = tf.math.sqrt(tf.cast(n_words, tf.float32))\n",
    "    return tf.reduce_sum(inputs, axis=1) / sqrt_n_words\n",
    "    \n",
    "    \n",
    "another_example = tf.constant([[[1., 2., 3.], [4., 5., 0.], [0., 0., 0.]],\n",
    "                               [[6., 0., 0.], [0., 0., 0.], [0., 0., 0.]]])\n",
    "aaa = compute_mean_embedding(another_example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    text_vectorization_layer,\n",
    "    keras.layers.Embedding(vocab_size, embedding_dim, name='embedding'),\n",
    "    keras.layers.Lambda(compute_mean_embedding),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_set, epochs=5, validation_data=valid_set)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9e299fd5d3720dea9f12703a5d4d3033d96b0089eb74c3ed594b0981e779212d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
